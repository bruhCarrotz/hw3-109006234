{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvwnt0y1eujKEAmkC3duxP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bruhCarrotz/hw3-109006234/blob/main/hw3_datascience.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing the DGL Package"
      ],
      "metadata": {
        "id": "GM4YmnjzanC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade dgl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWX7udDQmDJk",
        "outputId": "2c3fa78d-5284-4510-a754-528ecc62c154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgl\n",
            "  Downloading dgl-2.1.0-cp310-cp310-manylinux1_x86_64.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: torchdata>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (0.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2024.2.2)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata>=0.5.0->dgl) (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata>=0.5.0->dgl) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, dgl\n",
            "Successfully installed dgl-2.1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Replacing the Traditional Command Line Arguments"
      ],
      "metadata": {
        "id": "CRtggR3YatzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_arguments():\n",
        "    args = {\n",
        "        'epochs': 200,\n",
        "        'es_iters': 10,\n",
        "        'use_gpu': True\n",
        "    }\n",
        "\n",
        "    # Loop through command-line arguments\n",
        "    i = 1\n",
        "    while i < len(sys.argv):\n",
        "        if sys.argv[i] == '--epochs':\n",
        "            args['epochs'] = int(sys.argv[i + 1])\n",
        "            i += 1\n",
        "        elif sys.argv[i] == '--es_iters':\n",
        "            args['es_iters'] = int(sys.argv[i + 1])\n",
        "            i += 1\n",
        "        elif sys.argv[i] == '--use-gpu':\n",
        "            args['use_gpu'] = True\n",
        "        i += 1\n",
        "\n",
        "    return args"
      ],
      "metadata": {
        "id": "jpXQf_iFnlP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data from the Provided Dataset"
      ],
      "metadata": {
        "id": "oGAEb3M4a30L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl\n",
        "import sys\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"\n",
        "    * Load data from pickle file in folder `dataset`.\n",
        "    * No need to modify.\n",
        "\n",
        "    * test_labels is an array of length 1000 with each element being -1.\n",
        "    * train_mask, val_mask, and test_mask are used to indicate the index of each set of nodes.\n",
        "    \"\"\"\n",
        "    names = ['features', 'graph', 'num_classes',\n",
        "             'train_labels', 'val_labels', 'test_labels',\n",
        "             'train_mask', 'val_mask', 'test_mask']\n",
        "\n",
        "    objects = []\n",
        "    for i in range(len(names)):\n",
        "        with open(\"dataset/private_{}.pkl\".format(names[i]), 'rb') as f:\n",
        "            if sys.version_info > (3, 0):\n",
        "                objects.append(pkl.load(f, encoding='latin1'))\n",
        "            else:\n",
        "                objects.append(pkl.load(f))\n",
        "\n",
        "    features, graph, num_classes, \\\n",
        "    train_labels, val_labels, test_labels, \\\n",
        "    train_mask, val_mask, test_mask = tuple(objects)\n",
        "\n",
        "    return features, graph, num_classes, train_labels, val_labels, test_labels, train_mask, val_mask, test_mask"
      ],
      "metadata": {
        "id": "XmGWjSH9jelY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GAT Model"
      ],
      "metadata": {
        "id": "-ChzGbRga8YM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import dgl.function as fn\n",
        "from dgl.nn.pytorch import GATConv, GraphConv\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    \"\"\"\n",
        "    Baseline Model:\n",
        "    - A simple two-layer GCN model, similar to https://github.com/tkipf/pygcn\n",
        "    - Implement with DGL package\n",
        "    \"\"\"\n",
        "    def __init__(self, in_size, hid_size, out_size):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        # two-layer GCN\n",
        "        self.layers.append(\n",
        "            GraphConv(in_size, hid_size, activation=F.relu)\n",
        "        )\n",
        "        self.layers.append(GraphConv(hid_size, out_size))\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, g, features):\n",
        "        h = features\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if i != 0:\n",
        "                h = self.dropout(h)\n",
        "            h = layer(g, h)\n",
        "        return h\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    \"\"\"\n",
        "    Graph Attention Network (GAT) Model\n",
        "    \"\"\"\n",
        "    def __init__(self, in_size, hid_size, out_size, num_heads=1, num_layers=2, dropout=0.5):\n",
        "        super(GAT, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        # Input layer\n",
        "        self.layers.append(GATConv(in_size, hid_size, num_heads=num_heads))\n",
        "        # Hidden layers\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.layers.append(GATConv(hid_size * num_heads, hid_size, num_heads=num_heads))\n",
        "        # Output layer\n",
        "        self.layers.append(GATConv(hid_size * num_heads, out_size, num_heads=num_heads))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, g, features):\n",
        "        h = features\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            h = layer(g, h).flatten(1)\n",
        "            h = F.elu(h)\n",
        "            if i != len(self.layers) - 1:\n",
        "                h = self.dropout(h)\n",
        "        return h\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8m99vtVE3wx",
        "outputId": "229e68db-016b-436b-ebbc-366a44e9ea37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating and Training the Model"
      ],
      "metadata": {
        "id": "B8tqjlk1bG_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import dgl.function as fn\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def evaluate(g, features, labels, mask, model):\n",
        "    \"\"\"Evaluate model accuracy\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(g, features)\n",
        "        logits = logits[mask]\n",
        "        _, indices = torch.max(logits, dim=1)\n",
        "        correct = torch.sum(indices == labels)\n",
        "        return correct.item() * 1.0 / len(labels)\n",
        "\n",
        "def train(g, features, train_labels, val_labels, train_mask, val_mask, model, epochs, es_iters=None):\n",
        "\n",
        "    # define train/val samples, loss function and optimizer\n",
        "    loss_fcn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=5e-4)\n",
        "\n",
        "    # If early stopping criteria, initialize relevant parameters\n",
        "    if es_iters:\n",
        "        print(\"Early stopping monitoring on\")\n",
        "        loss_min = 1e8\n",
        "        es_i = 0\n",
        "\n",
        "    # training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        logits = model(g, features)\n",
        "        loss = loss_fcn(logits[train_mask], train_labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        acc = evaluate(g, features, val_labels, val_mask, model)\n",
        "        print(\n",
        "            \"Epoch {:05d} | Loss {:.4f} | Accuracy {:.4f} \".format(\n",
        "                epoch, loss.item(), acc\n",
        "            )\n",
        "        )\n",
        "\n",
        "        val_loss = loss_fcn(logits[val_mask], val_labels).item()\n",
        "        if es_iters:\n",
        "            if val_loss < loss_min:\n",
        "                loss_min = val_loss\n",
        "                es_i = 0\n",
        "            else:\n",
        "                es_i += 1\n",
        "\n",
        "            if es_i >= es_iters:\n",
        "                print(f\"Early stopping at epoch={epoch+1}\")\n",
        "                break"
      ],
      "metadata": {
        "id": "cySJk9MDbByV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Driver Function"
      ],
      "metadata": {
        "id": "WhUHpWCTbLqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    args = parse_arguments()\n",
        "\n",
        "    if args['use_gpu']:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    # Load data\n",
        "    features, graph, num_classes, \\\n",
        "    train_labels, val_labels, test_labels, \\\n",
        "    train_mask, val_mask, test_mask = load_data()\n",
        "\n",
        "    # Initialize the model (Baseline Model: GCN)\n",
        "    \"\"\"TODO: build your own model in model.py and replace GCN() with your model\"\"\"\n",
        "    in_size = features.shape[1]\n",
        "    out_size = num_classes\n",
        "    model = GAT(in_size, 32, out_size, num_heads=4, num_layers=2, dropout=0.1)\n",
        "    model.to(device)\n",
        "\n",
        "    # model training\n",
        "    print(\"Training...\")\n",
        "    train(graph, features, train_labels, val_labels, train_mask, val_mask, model, args['epochs'], args['es_iters'])\n",
        "\n",
        "    print(\"Testing...\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(graph, features)\n",
        "        logits = logits[test_mask]\n",
        "        _, indices = torch.max(logits, dim=1)\n",
        "\n",
        "    # Export predictions as csv file\n",
        "    print(\"Export predictions as csv file.\")\n",
        "    with open('output.csv', 'w') as f:\n",
        "        f.write('ID,Predict\\n')\n",
        "        for idx, pred in enumerate(indices):\n",
        "            f.write(f'{idx},{int(pred)}\\n')\n",
        "    # Please remember to upload your output.csv file to Kaggle for scoring"
      ],
      "metadata": {
        "id": "ePIC1_pkSh6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58b61f81-169a-4799-f83b-1b2c6a5e6daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n",
            "Early stopping monitoring on\n",
            "Epoch 00000 | Loss 2.4880 | Accuracy 0.5717 \n",
            "Epoch 00001 | Loss 2.2966 | Accuracy 0.6233 \n",
            "Epoch 00002 | Loss 2.0900 | Accuracy 0.6350 \n",
            "Epoch 00003 | Loss 1.8709 | Accuracy 0.6267 \n",
            "Epoch 00004 | Loss 1.6597 | Accuracy 0.6350 \n",
            "Epoch 00005 | Loss 1.4717 | Accuracy 0.6483 \n",
            "Epoch 00006 | Loss 1.2886 | Accuracy 0.6783 \n",
            "Epoch 00007 | Loss 1.1590 | Accuracy 0.7150 \n",
            "Epoch 00008 | Loss 1.0651 | Accuracy 0.7300 \n",
            "Epoch 00009 | Loss 0.9981 | Accuracy 0.7300 \n",
            "Epoch 00010 | Loss 0.9253 | Accuracy 0.7217 \n",
            "Epoch 00011 | Loss 0.8796 | Accuracy 0.7167 \n",
            "Epoch 00012 | Loss 0.8292 | Accuracy 0.7200 \n",
            "Epoch 00013 | Loss 0.8017 | Accuracy 0.7183 \n",
            "Epoch 00014 | Loss 0.7582 | Accuracy 0.7183 \n",
            "Epoch 00015 | Loss 0.7378 | Accuracy 0.7300 \n",
            "Epoch 00016 | Loss 0.7087 | Accuracy 0.7317 \n",
            "Epoch 00017 | Loss 0.6539 | Accuracy 0.7383 \n",
            "Epoch 00018 | Loss 0.6519 | Accuracy 0.7383 \n",
            "Epoch 00019 | Loss 0.6152 | Accuracy 0.7400 \n",
            "Epoch 00020 | Loss 0.5808 | Accuracy 0.7433 \n",
            "Epoch 00021 | Loss 0.5561 | Accuracy 0.7467 \n",
            "Epoch 00022 | Loss 0.5193 | Accuracy 0.7517 \n",
            "Epoch 00023 | Loss 0.4879 | Accuracy 0.7467 \n",
            "Epoch 00024 | Loss 0.4709 | Accuracy 0.7500 \n",
            "Epoch 00025 | Loss 0.4438 | Accuracy 0.7517 \n",
            "Epoch 00026 | Loss 0.4306 | Accuracy 0.7567 \n",
            "Epoch 00027 | Loss 0.3970 | Accuracy 0.7617 \n",
            "Epoch 00028 | Loss 0.3779 | Accuracy 0.7650 \n",
            "Epoch 00029 | Loss 0.3821 | Accuracy 0.7667 \n",
            "Epoch 00030 | Loss 0.3535 | Accuracy 0.7667 \n",
            "Epoch 00031 | Loss 0.3359 | Accuracy 0.7667 \n",
            "Epoch 00032 | Loss 0.3178 | Accuracy 0.7633 \n",
            "Epoch 00033 | Loss 0.2982 | Accuracy 0.7650 \n",
            "Epoch 00034 | Loss 0.3054 | Accuracy 0.7700 \n",
            "Epoch 00035 | Loss 0.2800 | Accuracy 0.7800 \n",
            "Epoch 00036 | Loss 0.2782 | Accuracy 0.7833 \n",
            "Epoch 00037 | Loss 0.2632 | Accuracy 0.7817 \n",
            "Epoch 00038 | Loss 0.2559 | Accuracy 0.7817 \n",
            "Epoch 00039 | Loss 0.2511 | Accuracy 0.7800 \n",
            "Epoch 00040 | Loss 0.2423 | Accuracy 0.7783 \n",
            "Epoch 00041 | Loss 0.2311 | Accuracy 0.7817 \n",
            "Epoch 00042 | Loss 0.2382 | Accuracy 0.7817 \n",
            "Epoch 00043 | Loss 0.2222 | Accuracy 0.7783 \n",
            "Epoch 00044 | Loss 0.2124 | Accuracy 0.7783 \n",
            "Epoch 00045 | Loss 0.1962 | Accuracy 0.7783 \n",
            "Epoch 00046 | Loss 0.1981 | Accuracy 0.7783 \n",
            "Epoch 00047 | Loss 0.1814 | Accuracy 0.7783 \n",
            "Epoch 00048 | Loss 0.1839 | Accuracy 0.7767 \n",
            "Epoch 00049 | Loss 0.1821 | Accuracy 0.7800 \n",
            "Epoch 00050 | Loss 0.1788 | Accuracy 0.7783 \n",
            "Epoch 00051 | Loss 0.1724 | Accuracy 0.7783 \n",
            "Epoch 00052 | Loss 0.1782 | Accuracy 0.7850 \n",
            "Epoch 00053 | Loss 0.1680 | Accuracy 0.7783 \n",
            "Epoch 00054 | Loss 0.1687 | Accuracy 0.7817 \n",
            "Epoch 00055 | Loss 0.1626 | Accuracy 0.7817 \n",
            "Epoch 00056 | Loss 0.1555 | Accuracy 0.7750 \n",
            "Epoch 00057 | Loss 0.1547 | Accuracy 0.7717 \n",
            "Epoch 00058 | Loss 0.1550 | Accuracy 0.7767 \n",
            "Epoch 00059 | Loss 0.1418 | Accuracy 0.7933 \n",
            "Epoch 00060 | Loss 0.1415 | Accuracy 0.7883 \n",
            "Epoch 00061 | Loss 0.1306 | Accuracy 0.7850 \n",
            "Epoch 00062 | Loss 0.1326 | Accuracy 0.7867 \n",
            "Epoch 00063 | Loss 0.1209 | Accuracy 0.7850 \n",
            "Epoch 00064 | Loss 0.1286 | Accuracy 0.7867 \n",
            "Epoch 00065 | Loss 0.1208 | Accuracy 0.7867 \n",
            "Epoch 00066 | Loss 0.1138 | Accuracy 0.7833 \n",
            "Epoch 00067 | Loss 0.1178 | Accuracy 0.7817 \n",
            "Epoch 00068 | Loss 0.1022 | Accuracy 0.7783 \n",
            "Epoch 00069 | Loss 0.1134 | Accuracy 0.7850 \n",
            "Epoch 00070 | Loss 0.1202 | Accuracy 0.7850 \n",
            "Epoch 00071 | Loss 0.1101 | Accuracy 0.7867 \n",
            "Epoch 00072 | Loss 0.0952 | Accuracy 0.7850 \n",
            "Epoch 00073 | Loss 0.1027 | Accuracy 0.7817 \n",
            "Epoch 00074 | Loss 0.1004 | Accuracy 0.7850 \n",
            "Epoch 00075 | Loss 0.0936 | Accuracy 0.7817 \n",
            "Epoch 00076 | Loss 0.0915 | Accuracy 0.7783 \n",
            "Epoch 00077 | Loss 0.1008 | Accuracy 0.7767 \n",
            "Epoch 00078 | Loss 0.0936 | Accuracy 0.7817 \n",
            "Epoch 00079 | Loss 0.0898 | Accuracy 0.7850 \n",
            "Epoch 00080 | Loss 0.0899 | Accuracy 0.7833 \n",
            "Early stopping at epoch=81\n",
            "Testing...\n",
            "Export predictions as csv file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding the Best Hyperparameter Tuning for GAT Model"
      ],
      "metadata": {
        "id": "3ebmUviAaJXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Define the search space for hyperparameters\n",
        "search_space = {\n",
        "    'hid_size': [8, 16, 32],\n",
        "    'num_heads': [2, 4, 8],\n",
        "    'num_layers': [2, 3, 4],\n",
        "    'dropout': [0.1, 0.01, 0.001]\n",
        "}\n",
        "\n",
        "# Set the number of trials\n",
        "num_trials = 10\n",
        "\n",
        "best_accuracy = 0\n",
        "best_hyperparameters = {}\n",
        "\n",
        "# Perform random search\n",
        "for i in range(num_trials):\n",
        "    # Randomly sample hyperparameters\n",
        "    hyperparameters = {\n",
        "        'in_size': features.shape[1],\n",
        "        'hid_size': random.choice(search_space['hid_size']),\n",
        "        'out_size': num_classes,\n",
        "        'num_heads': random.choice(search_space['num_heads']),\n",
        "        'num_layers': random.choice(search_space['num_layers']),\n",
        "        'dropout': random.choice(search_space['dropout'])\n",
        "    }\n",
        "\n",
        "    print(f\"Trial {i+1}/{num_trials}: Hyperparameters - {hyperparameters}\")\n",
        "\n",
        "    # Initialize and train the model with the sampled hyperparameters\n",
        "    model = GAT(**hyperparameters)\n",
        "    model.to(device)\n",
        "    train(graph, features, train_labels, val_labels, train_mask, val_mask, model, epochs=args['epochs'], es_iters=args['es_iters'])\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    accuracy = evaluate(graph, features, val_labels, val_mask, model)\n",
        "\n",
        "    print(f\"Validation Accuracy: {accuracy}\")\n",
        "\n",
        "    # Keep track of the best hyperparameters\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_hyperparameters = hyperparameters\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
        "print(\"Best Accuracy:\", best_accuracy)\n"
      ],
      "metadata": {
        "id": "yuBBu9zDXAjE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc8e31db-6cbf-4dc8-c23f-f9918083bc31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1/10: Hyperparameters - {'in_size': 478, 'hid_size': 8, 'out_size': 3, 'num_heads': 4, 'num_layers': 2, 'dropout': 0.1}\n",
            "Early stopping monitoring on\n",
            "Epoch 00000 | Loss 2.4919 | Accuracy 0.4317 \n",
            "Epoch 00001 | Loss 2.3950 | Accuracy 0.4050 \n",
            "Epoch 00002 | Loss 2.2990 | Accuracy 0.4050 \n",
            "Epoch 00003 | Loss 2.1962 | Accuracy 0.4050 \n",
            "Epoch 00004 | Loss 2.0853 | Accuracy 0.4050 \n",
            "Epoch 00005 | Loss 1.9813 | Accuracy 0.4050 \n",
            "Epoch 00006 | Loss 1.8685 | Accuracy 0.4050 \n",
            "Epoch 00007 | Loss 1.7666 | Accuracy 0.4050 \n",
            "Epoch 00008 | Loss 1.6686 | Accuracy 0.4050 \n",
            "Epoch 00009 | Loss 1.5667 | Accuracy 0.4050 \n",
            "Epoch 00010 | Loss 1.4890 | Accuracy 0.4050 \n",
            "Epoch 00011 | Loss 1.4056 | Accuracy 0.4050 \n",
            "Epoch 00012 | Loss 1.3172 | Accuracy 0.4050 \n",
            "Epoch 00013 | Loss 1.2786 | Accuracy 0.4100 \n",
            "Epoch 00014 | Loss 1.2107 | Accuracy 0.4100 \n",
            "Epoch 00015 | Loss 1.1576 | Accuracy 0.4200 \n",
            "Epoch 00016 | Loss 1.1020 | Accuracy 0.4700 \n",
            "Epoch 00017 | Loss 1.0721 | Accuracy 0.6433 \n",
            "Epoch 00018 | Loss 1.0457 | Accuracy 0.7133 \n",
            "Epoch 00019 | Loss 0.9979 | Accuracy 0.7167 \n",
            "Epoch 00020 | Loss 0.9776 | Accuracy 0.7183 \n",
            "Epoch 00021 | Loss 0.9623 | Accuracy 0.7267 \n",
            "Epoch 00022 | Loss 0.9080 | Accuracy 0.7233 \n",
            "Epoch 00023 | Loss 0.8855 | Accuracy 0.7250 \n",
            "Epoch 00024 | Loss 0.8634 | Accuracy 0.7250 \n",
            "Epoch 00025 | Loss 0.8546 | Accuracy 0.7233 \n",
            "Epoch 00026 | Loss 0.8230 | Accuracy 0.7183 \n",
            "Epoch 00027 | Loss 0.7937 | Accuracy 0.7250 \n",
            "Epoch 00028 | Loss 0.7897 | Accuracy 0.7233 \n",
            "Epoch 00029 | Loss 0.7681 | Accuracy 0.7267 \n",
            "Epoch 00030 | Loss 0.7853 | Accuracy 0.7433 \n",
            "Epoch 00031 | Loss 0.7503 | Accuracy 0.7417 \n",
            "Epoch 00032 | Loss 0.7274 | Accuracy 0.7417 \n",
            "Epoch 00033 | Loss 0.7262 | Accuracy 0.7433 \n",
            "Epoch 00034 | Loss 0.6956 | Accuracy 0.7467 \n",
            "Epoch 00035 | Loss 0.6946 | Accuracy 0.7417 \n",
            "Epoch 00036 | Loss 0.6578 | Accuracy 0.7417 \n",
            "Epoch 00037 | Loss 0.6316 | Accuracy 0.7383 \n",
            "Epoch 00038 | Loss 0.6574 | Accuracy 0.7383 \n",
            "Epoch 00039 | Loss 0.6208 | Accuracy 0.7333 \n",
            "Epoch 00040 | Loss 0.5906 | Accuracy 0.7333 \n",
            "Epoch 00041 | Loss 0.5761 | Accuracy 0.7333 \n",
            "Epoch 00042 | Loss 0.5893 | Accuracy 0.7400 \n",
            "Epoch 00043 | Loss 0.5643 | Accuracy 0.7433 \n",
            "Epoch 00044 | Loss 0.5337 | Accuracy 0.7417 \n",
            "Epoch 00045 | Loss 0.5275 | Accuracy 0.7533 \n",
            "Epoch 00046 | Loss 0.5357 | Accuracy 0.7567 \n",
            "Epoch 00047 | Loss 0.4966 | Accuracy 0.7600 \n",
            "Epoch 00048 | Loss 0.5040 | Accuracy 0.7583 \n",
            "Epoch 00049 | Loss 0.4626 | Accuracy 0.7600 \n",
            "Epoch 00050 | Loss 0.4528 | Accuracy 0.7633 \n",
            "Epoch 00051 | Loss 0.4611 | Accuracy 0.7667 \n",
            "Epoch 00052 | Loss 0.4549 | Accuracy 0.7700 \n",
            "Epoch 00053 | Loss 0.4339 | Accuracy 0.7717 \n",
            "Epoch 00054 | Loss 0.4286 | Accuracy 0.7683 \n",
            "Epoch 00055 | Loss 0.4176 | Accuracy 0.7633 \n",
            "Epoch 00056 | Loss 0.4201 | Accuracy 0.7650 \n",
            "Epoch 00057 | Loss 0.4013 | Accuracy 0.7650 \n",
            "Epoch 00058 | Loss 0.3937 | Accuracy 0.7650 \n",
            "Epoch 00059 | Loss 0.3714 | Accuracy 0.7683 \n",
            "Epoch 00060 | Loss 0.3792 | Accuracy 0.7667 \n",
            "Epoch 00061 | Loss 0.3804 | Accuracy 0.7667 \n",
            "Epoch 00062 | Loss 0.3739 | Accuracy 0.7700 \n",
            "Epoch 00063 | Loss 0.3570 | Accuracy 0.7700 \n",
            "Epoch 00064 | Loss 0.3499 | Accuracy 0.7717 \n",
            "Epoch 00065 | Loss 0.3230 | Accuracy 0.7717 \n",
            "Epoch 00066 | Loss 0.3491 | Accuracy 0.7750 \n",
            "Epoch 00067 | Loss 0.3265 | Accuracy 0.7767 \n",
            "Epoch 00068 | Loss 0.3222 | Accuracy 0.7783 \n",
            "Epoch 00069 | Loss 0.3264 | Accuracy 0.7783 \n",
            "Epoch 00070 | Loss 0.3095 | Accuracy 0.7767 \n",
            "Epoch 00071 | Loss 0.3173 | Accuracy 0.7800 \n",
            "Epoch 00072 | Loss 0.2947 | Accuracy 0.7800 \n",
            "Epoch 00073 | Loss 0.3005 | Accuracy 0.7800 \n",
            "Epoch 00074 | Loss 0.3027 | Accuracy 0.7800 \n",
            "Epoch 00075 | Loss 0.2932 | Accuracy 0.7817 \n",
            "Epoch 00076 | Loss 0.2610 | Accuracy 0.7817 \n",
            "Epoch 00077 | Loss 0.2695 | Accuracy 0.7817 \n",
            "Epoch 00078 | Loss 0.2574 | Accuracy 0.7850 \n",
            "Epoch 00079 | Loss 0.2524 | Accuracy 0.7833 \n",
            "Epoch 00080 | Loss 0.2574 | Accuracy 0.7850 \n",
            "Epoch 00081 | Loss 0.2403 | Accuracy 0.7850 \n",
            "Epoch 00082 | Loss 0.2495 | Accuracy 0.7850 \n",
            "Epoch 00083 | Loss 0.2421 | Accuracy 0.7850 \n",
            "Epoch 00084 | Loss 0.2435 | Accuracy 0.7783 \n",
            "Epoch 00085 | Loss 0.2432 | Accuracy 0.7783 \n",
            "Epoch 00086 | Loss 0.2194 | Accuracy 0.7783 \n",
            "Epoch 00087 | Loss 0.2418 | Accuracy 0.7783 \n",
            "Epoch 00088 | Loss 0.2373 | Accuracy 0.7750 \n",
            "Epoch 00089 | Loss 0.2343 | Accuracy 0.7717 \n",
            "Epoch 00090 | Loss 0.2107 | Accuracy 0.7717 \n",
            "Epoch 00091 | Loss 0.2463 | Accuracy 0.7733 \n",
            "Epoch 00092 | Loss 0.2270 | Accuracy 0.7733 \n",
            "Epoch 00093 | Loss 0.1983 | Accuracy 0.7800 \n",
            "Epoch 00094 | Loss 0.2053 | Accuracy 0.7800 \n",
            "Early stopping at epoch=95\n",
            "Validation Accuracy: 0.78\n",
            "Trial 2/10: Hyperparameters - {'in_size': 478, 'hid_size': 16, 'out_size': 3, 'num_heads': 2, 'num_layers': 4, 'dropout': 0.01}\n",
            "Early stopping monitoring on\n",
            "Epoch 00000 | Loss 1.7862 | Accuracy 0.4967 \n",
            "Epoch 00001 | Loss 1.5884 | Accuracy 0.5767 \n",
            "Epoch 00002 | Loss 1.3892 | Accuracy 0.5733 \n",
            "Epoch 00003 | Loss 1.1930 | Accuracy 0.5800 \n",
            "Epoch 00004 | Loss 1.0469 | Accuracy 0.6950 \n",
            "Epoch 00005 | Loss 0.9453 | Accuracy 0.7050 \n",
            "Epoch 00006 | Loss 0.8601 | Accuracy 0.7067 \n",
            "Epoch 00007 | Loss 0.7874 | Accuracy 0.6833 \n",
            "Epoch 00008 | Loss 0.7150 | Accuracy 0.6867 \n",
            "Epoch 00009 | Loss 0.6495 | Accuracy 0.7067 \n",
            "Epoch 00010 | Loss 0.5920 | Accuracy 0.7100 \n",
            "Epoch 00011 | Loss 0.5109 | Accuracy 0.7083 \n",
            "Epoch 00012 | Loss 0.4749 | Accuracy 0.7167 \n",
            "Epoch 00013 | Loss 0.4308 | Accuracy 0.7233 \n",
            "Epoch 00014 | Loss 0.3795 | Accuracy 0.7317 \n",
            "Epoch 00015 | Loss 0.3187 | Accuracy 0.7383 \n",
            "Epoch 00016 | Loss 0.2958 | Accuracy 0.7300 \n",
            "Epoch 00017 | Loss 0.2497 | Accuracy 0.7467 \n",
            "Epoch 00018 | Loss 0.2118 | Accuracy 0.7417 \n",
            "Epoch 00019 | Loss 0.1888 | Accuracy 0.7400 \n",
            "Epoch 00020 | Loss 0.1718 | Accuracy 0.7383 \n",
            "Epoch 00021 | Loss 0.1474 | Accuracy 0.7483 \n",
            "Epoch 00022 | Loss 0.1357 | Accuracy 0.7533 \n",
            "Epoch 00023 | Loss 0.1016 | Accuracy 0.7550 \n",
            "Epoch 00024 | Loss 0.0932 | Accuracy 0.7767 \n",
            "Early stopping at epoch=25\n",
            "Validation Accuracy: 0.7766666666666666\n",
            "Trial 3/10: Hyperparameters - {'in_size': 478, 'hid_size': 16, 'out_size': 3, 'num_heads': 4, 'num_layers': 2, 'dropout': 0.001}\n",
            "Early stopping monitoring on\n",
            "Epoch 00000 | Loss 2.4946 | Accuracy 0.5383 \n",
            "Epoch 00001 | Loss 2.3530 | Accuracy 0.5433 \n",
            "Epoch 00002 | Loss 2.2054 | Accuracy 0.5517 \n",
            "Epoch 00003 | Loss 2.0480 | Accuracy 0.5533 \n",
            "Epoch 00004 | Loss 1.8863 | Accuracy 0.5550 \n",
            "Epoch 00005 | Loss 1.7277 | Accuracy 0.5533 \n",
            "Epoch 00006 | Loss 1.5783 | Accuracy 0.5600 \n",
            "Epoch 00007 | Loss 1.4455 | Accuracy 0.5633 \n",
            "Epoch 00008 | Loss 1.3346 | Accuracy 0.5733 \n",
            "Epoch 00009 | Loss 1.2413 | Accuracy 0.5783 \n",
            "Epoch 00010 | Loss 1.1633 | Accuracy 0.5883 \n",
            "Epoch 00011 | Loss 1.0995 | Accuracy 0.5883 \n",
            "Epoch 00012 | Loss 1.0422 | Accuracy 0.5967 \n",
            "Epoch 00013 | Loss 0.9923 | Accuracy 0.6600 \n",
            "Epoch 00014 | Loss 0.9474 | Accuracy 0.7100 \n",
            "Epoch 00015 | Loss 0.9068 | Accuracy 0.7383 \n",
            "Epoch 00016 | Loss 0.8743 | Accuracy 0.7367 \n",
            "Epoch 00017 | Loss 0.8457 | Accuracy 0.7300 \n",
            "Epoch 00018 | Loss 0.8212 | Accuracy 0.7183 \n",
            "Epoch 00019 | Loss 0.7978 | Accuracy 0.7083 \n",
            "Epoch 00020 | Loss 0.7741 | Accuracy 0.7233 \n",
            "Epoch 00021 | Loss 0.7463 | Accuracy 0.7233 \n",
            "Epoch 00022 | Loss 0.7206 | Accuracy 0.7200 \n",
            "Epoch 00023 | Loss 0.6978 | Accuracy 0.7233 \n",
            "Epoch 00024 | Loss 0.6707 | Accuracy 0.7300 \n",
            "Epoch 00025 | Loss 0.6480 | Accuracy 0.7283 \n",
            "Epoch 00026 | Loss 0.6276 | Accuracy 0.7317 \n",
            "Epoch 00027 | Loss 0.6064 | Accuracy 0.7350 \n",
            "Epoch 00028 | Loss 0.5865 | Accuracy 0.7417 \n",
            "Epoch 00029 | Loss 0.5652 | Accuracy 0.7483 \n",
            "Epoch 00030 | Loss 0.5481 | Accuracy 0.7467 \n",
            "Epoch 00031 | Loss 0.5250 | Accuracy 0.7500 \n",
            "Epoch 00032 | Loss 0.5072 | Accuracy 0.7517 \n",
            "Epoch 00033 | Loss 0.4873 | Accuracy 0.7583 \n",
            "Epoch 00034 | Loss 0.4709 | Accuracy 0.7650 \n",
            "Epoch 00035 | Loss 0.4551 | Accuracy 0.7583 \n",
            "Epoch 00036 | Loss 0.4381 | Accuracy 0.7617 \n",
            "Epoch 00037 | Loss 0.4227 | Accuracy 0.7633 \n",
            "Epoch 00038 | Loss 0.4074 | Accuracy 0.7667 \n",
            "Epoch 00039 | Loss 0.3920 | Accuracy 0.7700 \n",
            "Epoch 00040 | Loss 0.3786 | Accuracy 0.7683 \n",
            "Epoch 00041 | Loss 0.3646 | Accuracy 0.7733 \n",
            "Epoch 00042 | Loss 0.3555 | Accuracy 0.7767 \n",
            "Epoch 00043 | Loss 0.3423 | Accuracy 0.7817 \n",
            "Epoch 00044 | Loss 0.3313 | Accuracy 0.7850 \n",
            "Epoch 00045 | Loss 0.3217 | Accuracy 0.7850 \n",
            "Epoch 00046 | Loss 0.3129 | Accuracy 0.7800 \n",
            "Epoch 00047 | Loss 0.3023 | Accuracy 0.7817 \n",
            "Epoch 00048 | Loss 0.2933 | Accuracy 0.7833 \n",
            "Epoch 00049 | Loss 0.2843 | Accuracy 0.7817 \n",
            "Epoch 00050 | Loss 0.2775 | Accuracy 0.7850 \n",
            "Epoch 00051 | Loss 0.2697 | Accuracy 0.7817 \n",
            "Epoch 00052 | Loss 0.2625 | Accuracy 0.7817 \n",
            "Epoch 00053 | Loss 0.2559 | Accuracy 0.7817 \n",
            "Epoch 00054 | Loss 0.2485 | Accuracy 0.7783 \n",
            "Epoch 00055 | Loss 0.2447 | Accuracy 0.7817 \n",
            "Epoch 00056 | Loss 0.2364 | Accuracy 0.7800 \n",
            "Epoch 00057 | Loss 0.2308 | Accuracy 0.7817 \n",
            "Epoch 00058 | Loss 0.2267 | Accuracy 0.7800 \n",
            "Epoch 00059 | Loss 0.2230 | Accuracy 0.7783 \n",
            "Epoch 00060 | Loss 0.2171 | Accuracy 0.7783 \n",
            "Epoch 00061 | Loss 0.2112 | Accuracy 0.7783 \n",
            "Epoch 00062 | Loss 0.2105 | Accuracy 0.7783 \n",
            "Epoch 00063 | Loss 0.2038 | Accuracy 0.7767 \n",
            "Epoch 00064 | Loss 0.1994 | Accuracy 0.7750 \n",
            "Epoch 00065 | Loss 0.1976 | Accuracy 0.7783 \n",
            "Epoch 00066 | Loss 0.1915 | Accuracy 0.7783 \n",
            "Epoch 00067 | Loss 0.1880 | Accuracy 0.7783 \n",
            "Epoch 00068 | Loss 0.1845 | Accuracy 0.7783 \n",
            "Epoch 00069 | Loss 0.1804 | Accuracy 0.7783 \n",
            "Epoch 00070 | Loss 0.1763 | Accuracy 0.7767 \n",
            "Epoch 00071 | Loss 0.1733 | Accuracy 0.7767 \n",
            "Epoch 00072 | Loss 0.1705 | Accuracy 0.7783 \n",
            "Epoch 00073 | Loss 0.1668 | Accuracy 0.7783 \n",
            "Epoch 00074 | Loss 0.1632 | Accuracy 0.7767 \n",
            "Epoch 00075 | Loss 0.1608 | Accuracy 0.7800 \n",
            "Epoch 00076 | Loss 0.1564 | Accuracy 0.7800 \n",
            "Epoch 00077 | Loss 0.1519 | Accuracy 0.7800 \n",
            "Epoch 00078 | Loss 0.1484 | Accuracy 0.7783 \n",
            "Epoch 00079 | Loss 0.1461 | Accuracy 0.7767 \n",
            "Epoch 00080 | Loss 0.1423 | Accuracy 0.7767 \n",
            "Epoch 00081 | Loss 0.1405 | Accuracy 0.7783 \n",
            "Epoch 00082 | Loss 0.1377 | Accuracy 0.7767 \n",
            "Epoch 00083 | Loss 0.1349 | Accuracy 0.7767 \n",
            "Epoch 00084 | Loss 0.1320 | Accuracy 0.7783 \n",
            "Epoch 00085 | Loss 0.1316 | Accuracy 0.7783 \n",
            "Epoch 00086 | Loss 0.1272 | Accuracy 0.7783 \n",
            "Epoch 00087 | Loss 0.1267 | Accuracy 0.7767 \n",
            "Epoch 00088 | Loss 0.1241 | Accuracy 0.7750 \n",
            "Epoch 00089 | Loss 0.1228 | Accuracy 0.7750 \n",
            "Epoch 00090 | Loss 0.1209 | Accuracy 0.7733 \n",
            "Epoch 00091 | Loss 0.1180 | Accuracy 0.7733 \n",
            "Epoch 00092 | Loss 0.1169 | Accuracy 0.7733 \n",
            "Epoch 00093 | Loss 0.1149 | Accuracy 0.7733 \n",
            "Epoch 00094 | Loss 0.1131 | Accuracy 0.7750 \n",
            "Epoch 00095 | Loss 0.1113 | Accuracy 0.7750 \n",
            "Epoch 00096 | Loss 0.1104 | Accuracy 0.7750 \n",
            "Epoch 00097 | Loss 0.1086 | Accuracy 0.7750 \n",
            "Epoch 00098 | Loss 0.1077 | Accuracy 0.7717 \n",
            "Epoch 00099 | Loss 0.1060 | Accuracy 0.7733 \n",
            "Epoch 00100 | Loss 0.1049 | Accuracy 0.7733 \n",
            "Epoch 00101 | Loss 0.1028 | Accuracy 0.7750 \n",
            "Epoch 00102 | Loss 0.1018 | Accuracy 0.7750 \n",
            "Epoch 00103 | Loss 0.1028 | Accuracy 0.7750 \n",
            "Epoch 00104 | Loss 0.0987 | Accuracy 0.7750 \n",
            "Epoch 00105 | Loss 0.0984 | Accuracy 0.7750 \n",
            "Epoch 00106 | Loss 0.0972 | Accuracy 0.7750 \n",
            "Early stopping at epoch=107\n",
            "Validation Accuracy: 0.775\n",
            "Trial 4/10: Hyperparameters - {'in_size': 478, 'hid_size': 8, 'out_size': 3, 'num_heads': 4, 'num_layers': 2, 'dropout': 0.01}\n",
            "Early stopping monitoring on\n",
            "Epoch 00000 | Loss 2.4819 | Accuracy 0.4617 \n",
            "Epoch 00001 | Loss 2.3871 | Accuracy 0.4783 \n",
            "Epoch 00002 | Loss 2.2897 | Accuracy 0.4833 \n",
            "Epoch 00003 | Loss 2.1850 | Accuracy 0.4767 \n",
            "Epoch 00004 | Loss 2.0779 | Accuracy 0.4700 \n",
            "Epoch 00005 | Loss 1.9659 | Accuracy 0.4717 \n",
            "Epoch 00006 | Loss 1.8559 | Accuracy 0.4633 \n",
            "Epoch 00007 | Loss 1.7426 | Accuracy 0.4733 \n",
            "Epoch 00008 | Loss 1.6389 | Accuracy 0.4900 \n",
            "Epoch 00009 | Loss 1.5417 | Accuracy 0.5033 \n",
            "Epoch 00010 | Loss 1.4489 | Accuracy 0.5267 \n",
            "Epoch 00011 | Loss 1.3738 | Accuracy 0.5650 \n",
            "Epoch 00012 | Loss 1.2964 | Accuracy 0.5883 \n",
            "Epoch 00013 | Loss 1.2358 | Accuracy 0.6167 \n",
            "Epoch 00014 | Loss 1.1793 | Accuracy 0.6567 \n",
            "Epoch 00015 | Loss 1.1361 | Accuracy 0.6983 \n",
            "Epoch 00016 | Loss 1.0906 | Accuracy 0.7167 \n",
            "Epoch 00017 | Loss 1.0586 | Accuracy 0.7317 \n",
            "Epoch 00018 | Loss 1.0276 | Accuracy 0.7233 \n",
            "Epoch 00019 | Loss 1.0023 | Accuracy 0.7183 \n",
            "Epoch 00020 | Loss 0.9832 | Accuracy 0.7150 \n",
            "Epoch 00021 | Loss 0.9535 | Accuracy 0.7150 \n",
            "Epoch 00022 | Loss 0.9351 | Accuracy 0.7150 \n",
            "Epoch 00023 | Loss 0.9074 | Accuracy 0.7167 \n",
            "Epoch 00024 | Loss 0.8915 | Accuracy 0.7150 \n",
            "Epoch 00025 | Loss 0.8745 | Accuracy 0.7167 \n",
            "Epoch 00026 | Loss 0.8537 | Accuracy 0.7183 \n",
            "Epoch 00027 | Loss 0.8358 | Accuracy 0.7200 \n",
            "Epoch 00028 | Loss 0.8210 | Accuracy 0.7250 \n",
            "Epoch 00029 | Loss 0.8028 | Accuracy 0.7217 \n",
            "Epoch 00030 | Loss 0.7867 | Accuracy 0.7217 \n",
            "Epoch 00031 | Loss 0.7603 | Accuracy 0.7217 \n",
            "Epoch 00032 | Loss 0.7469 | Accuracy 0.7233 \n",
            "Epoch 00033 | Loss 0.7368 | Accuracy 0.7283 \n",
            "Epoch 00034 | Loss 0.7055 | Accuracy 0.7300 \n",
            "Epoch 00035 | Loss 0.6988 | Accuracy 0.7300 \n",
            "Epoch 00036 | Loss 0.6837 | Accuracy 0.7350 \n",
            "Epoch 00037 | Loss 0.6534 | Accuracy 0.7350 \n",
            "Epoch 00038 | Loss 0.6422 | Accuracy 0.7350 \n",
            "Epoch 00039 | Loss 0.6266 | Accuracy 0.7367 \n",
            "Epoch 00040 | Loss 0.6080 | Accuracy 0.7367 \n",
            "Epoch 00041 | Loss 0.5949 | Accuracy 0.7383 \n",
            "Epoch 00042 | Loss 0.5819 | Accuracy 0.7467 \n",
            "Epoch 00043 | Loss 0.5579 | Accuracy 0.7483 \n",
            "Epoch 00044 | Loss 0.5501 | Accuracy 0.7533 \n",
            "Epoch 00045 | Loss 0.5243 | Accuracy 0.7550 \n",
            "Epoch 00046 | Loss 0.5067 | Accuracy 0.7550 \n",
            "Epoch 00047 | Loss 0.4992 | Accuracy 0.7583 \n",
            "Epoch 00048 | Loss 0.4851 | Accuracy 0.7583 \n",
            "Epoch 00049 | Loss 0.4842 | Accuracy 0.7583 \n",
            "Epoch 00050 | Loss 0.4640 | Accuracy 0.7567 \n",
            "Epoch 00051 | Loss 0.4488 | Accuracy 0.7567 \n",
            "Epoch 00052 | Loss 0.4479 | Accuracy 0.7567 \n",
            "Epoch 00053 | Loss 0.4295 | Accuracy 0.7600 \n",
            "Epoch 00054 | Loss 0.4149 | Accuracy 0.7633 \n",
            "Epoch 00055 | Loss 0.4036 | Accuracy 0.7617 \n",
            "Epoch 00056 | Loss 0.4006 | Accuracy 0.7617 \n",
            "Epoch 00057 | Loss 0.3840 | Accuracy 0.7650 \n",
            "Epoch 00058 | Loss 0.3728 | Accuracy 0.7733 \n",
            "Epoch 00059 | Loss 0.3567 | Accuracy 0.7750 \n",
            "Epoch 00060 | Loss 0.3537 | Accuracy 0.7767 \n",
            "Epoch 00061 | Loss 0.3505 | Accuracy 0.7767 \n",
            "Epoch 00062 | Loss 0.3405 | Accuracy 0.7783 \n",
            "Epoch 00063 | Loss 0.3295 | Accuracy 0.7833 \n",
            "Epoch 00064 | Loss 0.3276 | Accuracy 0.7833 \n",
            "Epoch 00065 | Loss 0.3163 | Accuracy 0.7833 \n",
            "Epoch 00066 | Loss 0.3035 | Accuracy 0.7833 \n",
            "Epoch 00067 | Loss 0.3067 | Accuracy 0.7833 \n",
            "Epoch 00068 | Loss 0.2952 | Accuracy 0.7833 \n",
            "Epoch 00069 | Loss 0.2909 | Accuracy 0.7833 \n",
            "Epoch 00070 | Loss 0.2865 | Accuracy 0.7833 \n",
            "Epoch 00071 | Loss 0.2743 | Accuracy 0.7817 \n",
            "Epoch 00072 | Loss 0.2715 | Accuracy 0.7833 \n",
            "Epoch 00073 | Loss 0.2653 | Accuracy 0.7833 \n",
            "Epoch 00074 | Loss 0.2630 | Accuracy 0.7833 \n",
            "Epoch 00075 | Loss 0.2566 | Accuracy 0.7833 \n",
            "Epoch 00076 | Loss 0.2512 | Accuracy 0.7817 \n",
            "Epoch 00077 | Loss 0.2413 | Accuracy 0.7833 \n",
            "Epoch 00078 | Loss 0.2417 | Accuracy 0.7883 \n",
            "Epoch 00079 | Loss 0.2349 | Accuracy 0.7850 \n",
            "Epoch 00080 | Loss 0.2277 | Accuracy 0.7833 \n",
            "Epoch 00081 | Loss 0.2258 | Accuracy 0.7833 \n",
            "Epoch 00082 | Loss 0.2251 | Accuracy 0.7833 \n",
            "Epoch 00083 | Loss 0.2221 | Accuracy 0.7850 \n",
            "Epoch 00084 | Loss 0.2091 | Accuracy 0.7867 \n",
            "Epoch 00085 | Loss 0.2072 | Accuracy 0.7867 \n",
            "Epoch 00086 | Loss 0.2044 | Accuracy 0.7833 \n",
            "Epoch 00087 | Loss 0.2037 | Accuracy 0.7800 \n",
            "Epoch 00088 | Loss 0.1957 | Accuracy 0.7750 \n",
            "Epoch 00089 | Loss 0.1934 | Accuracy 0.7750 \n",
            "Epoch 00090 | Loss 0.1864 | Accuracy 0.7750 \n",
            "Epoch 00091 | Loss 0.1821 | Accuracy 0.7750 \n",
            "Epoch 00092 | Loss 0.1776 | Accuracy 0.7783 \n",
            "Epoch 00093 | Loss 0.1743 | Accuracy 0.7833 \n",
            "Epoch 00094 | Loss 0.1697 | Accuracy 0.7833 \n",
            "Epoch 00095 | Loss 0.1715 | Accuracy 0.7867 \n",
            "Epoch 00096 | Loss 0.1686 | Accuracy 0.7867 \n",
            "Epoch 00097 | Loss 0.1591 | Accuracy 0.7850 \n",
            "Epoch 00098 | Loss 0.1561 | Accuracy 0.7833 \n",
            "Epoch 00099 | Loss 0.1642 | Accuracy 0.7850 \n",
            "Epoch 00100 | Loss 0.1551 | Accuracy 0.7833 \n",
            "Epoch 00101 | Loss 0.1582 | Accuracy 0.7800 \n",
            "Epoch 00102 | Loss 0.1543 | Accuracy 0.7817 \n",
            "Epoch 00103 | Loss 0.1504 | Accuracy 0.7817 \n",
            "Epoch 00104 | Loss 0.1396 | Accuracy 0.7850 \n",
            "Epoch 00105 | Loss 0.1429 | Accuracy 0.7833 \n",
            "Epoch 00106 | Loss 0.1378 | Accuracy 0.7833 \n",
            "Epoch 00107 | Loss 0.1408 | Accuracy 0.7833 \n",
            "Epoch 00108 | Loss 0.1379 | Accuracy 0.7817 \n",
            "Epoch 00109 | Loss 0.1392 | Accuracy 0.7817 \n",
            "Epoch 00110 | Loss 0.1301 | Accuracy 0.7817 \n",
            "Epoch 00111 | Loss 0.1323 | Accuracy 0.7833 \n",
            "Epoch 00112 | Loss 0.1321 | Accuracy 0.7833 \n",
            "Epoch 00113 | Loss 0.1266 | Accuracy 0.7783 \n",
            "Epoch 00114 | Loss 0.1295 | Accuracy 0.7767 \n",
            "Epoch 00115 | Loss 0.1279 | Accuracy 0.7783 \n",
            "Early stopping at epoch=116\n",
            "Validation Accuracy: 0.7783333333333333\n",
            "Trial 5/10: Hyperparameters - {'in_size': 478, 'hid_size': 32, 'out_size': 3, 'num_heads': 8, 'num_layers': 2, 'dropout': 0.001}\n",
            "Early stopping monitoring on\n",
            "Epoch 00000 | Loss 3.1820 | Accuracy 0.6867 \n",
            "Epoch 00001 | Loss 2.8809 | Accuracy 0.7083 \n",
            "Epoch 00002 | Loss 2.5273 | Accuracy 0.7100 \n",
            "Epoch 00003 | Loss 2.1258 | Accuracy 0.7050 \n",
            "Epoch 00004 | Loss 1.7272 | Accuracy 0.7067 \n",
            "Epoch 00005 | Loss 1.3916 | Accuracy 0.7117 \n",
            "Epoch 00006 | Loss 1.1548 | Accuracy 0.7133 \n",
            "Epoch 00007 | Loss 1.0030 | Accuracy 0.7183 \n",
            "Epoch 00008 | Loss 0.9065 | Accuracy 0.7183 \n",
            "Epoch 00009 | Loss 0.8398 | Accuracy 0.7183 \n",
            "Epoch 00010 | Loss 0.7863 | Accuracy 0.7233 \n",
            "Epoch 00011 | Loss 0.7390 | Accuracy 0.7250 \n",
            "Epoch 00012 | Loss 0.6976 | Accuracy 0.7317 \n",
            "Epoch 00013 | Loss 0.6568 | Accuracy 0.7333 \n",
            "Epoch 00014 | Loss 0.6178 | Accuracy 0.7383 \n",
            "Epoch 00015 | Loss 0.5802 | Accuracy 0.7383 \n",
            "Epoch 00016 | Loss 0.5461 | Accuracy 0.7400 \n",
            "Epoch 00017 | Loss 0.5125 | Accuracy 0.7450 \n",
            "Epoch 00018 | Loss 0.4819 | Accuracy 0.7517 \n",
            "Epoch 00019 | Loss 0.4533 | Accuracy 0.7533 \n",
            "Epoch 00020 | Loss 0.4256 | Accuracy 0.7583 \n",
            "Epoch 00021 | Loss 0.4009 | Accuracy 0.7583 \n",
            "Epoch 00022 | Loss 0.3788 | Accuracy 0.7583 \n",
            "Epoch 00023 | Loss 0.3573 | Accuracy 0.7600 \n",
            "Epoch 00024 | Loss 0.3379 | Accuracy 0.7650 \n",
            "Epoch 00025 | Loss 0.3191 | Accuracy 0.7633 \n",
            "Epoch 00026 | Loss 0.3028 | Accuracy 0.7667 \n",
            "Epoch 00027 | Loss 0.2883 | Accuracy 0.7700 \n",
            "Epoch 00028 | Loss 0.2745 | Accuracy 0.7750 \n",
            "Epoch 00029 | Loss 0.2606 | Accuracy 0.7833 \n",
            "Epoch 00030 | Loss 0.2485 | Accuracy 0.7767 \n",
            "Epoch 00031 | Loss 0.2393 | Accuracy 0.7767 \n",
            "Epoch 00032 | Loss 0.2306 | Accuracy 0.7817 \n",
            "Epoch 00033 | Loss 0.2216 | Accuracy 0.7833 \n",
            "Epoch 00034 | Loss 0.2140 | Accuracy 0.7800 \n",
            "Epoch 00035 | Loss 0.2083 | Accuracy 0.7833 \n",
            "Epoch 00036 | Loss 0.2009 | Accuracy 0.7817 \n",
            "Epoch 00037 | Loss 0.1950 | Accuracy 0.7783 \n",
            "Epoch 00038 | Loss 0.1915 | Accuracy 0.7750 \n",
            "Epoch 00039 | Loss 0.1869 | Accuracy 0.7750 \n",
            "Epoch 00040 | Loss 0.1818 | Accuracy 0.7733 \n",
            "Epoch 00041 | Loss 0.1776 | Accuracy 0.7750 \n",
            "Epoch 00042 | Loss 0.1738 | Accuracy 0.7750 \n",
            "Epoch 00043 | Loss 0.1708 | Accuracy 0.7783 \n",
            "Epoch 00044 | Loss 0.1678 | Accuracy 0.7800 \n",
            "Epoch 00045 | Loss 0.1643 | Accuracy 0.7800 \n",
            "Epoch 00046 | Loss 0.1608 | Accuracy 0.7800 \n",
            "Epoch 00047 | Loss 0.1586 | Accuracy 0.7817 \n",
            "Epoch 00048 | Loss 0.1557 | Accuracy 0.7833 \n",
            "Epoch 00049 | Loss 0.1535 | Accuracy 0.7817 \n",
            "Epoch 00050 | Loss 0.1507 | Accuracy 0.7817 \n",
            "Epoch 00051 | Loss 0.1482 | Accuracy 0.7817 \n",
            "Epoch 00052 | Loss 0.1455 | Accuracy 0.7817 \n",
            "Epoch 00053 | Loss 0.1427 | Accuracy 0.7817 \n",
            "Epoch 00054 | Loss 0.1413 | Accuracy 0.7850 \n",
            "Epoch 00055 | Loss 0.1382 | Accuracy 0.7800 \n",
            "Epoch 00056 | Loss 0.1356 | Accuracy 0.7817 \n",
            "Epoch 00057 | Loss 0.1338 | Accuracy 0.7817 \n",
            "Epoch 00058 | Loss 0.1311 | Accuracy 0.7833 \n",
            "Epoch 00059 | Loss 0.1302 | Accuracy 0.7833 \n",
            "Epoch 00060 | Loss 0.1270 | Accuracy 0.7817 \n",
            "Epoch 00061 | Loss 0.1250 | Accuracy 0.7833 \n",
            "Epoch 00062 | Loss 0.1239 | Accuracy 0.7833 \n",
            "Epoch 00063 | Loss 0.1219 | Accuracy 0.7833 \n",
            "Epoch 00064 | Loss 0.1194 | Accuracy 0.7833 \n",
            "Epoch 00065 | Loss 0.1184 | Accuracy 0.7817 \n",
            "Epoch 00066 | Loss 0.1164 | Accuracy 0.7850 \n",
            "Epoch 00067 | Loss 0.1140 | Accuracy 0.7817 \n",
            "Early stopping at epoch=68\n",
            "Validation Accuracy: 0.7816666666666666\n",
            "Trial 6/10: Hyperparameters - {'in_size': 478, 'hid_size': 16, 'out_size': 3, 'num_heads': 2, 'num_layers': 4, 'dropout': 0.001}\n",
            "Early stopping monitoring on\n",
            "Epoch 00000 | Loss 1.7933 | Accuracy 0.5033 \n",
            "Epoch 00001 | Loss 1.6026 | Accuracy 0.6117 \n",
            "Epoch 00002 | Loss 1.4299 | Accuracy 0.7067 \n",
            "Epoch 00003 | Loss 1.2744 | Accuracy 0.7033 \n",
            "Epoch 00004 | Loss 1.1480 | Accuracy 0.6833 \n",
            "Epoch 00005 | Loss 1.0552 | Accuracy 0.7183 \n",
            "Epoch 00006 | Loss 0.9825 | Accuracy 0.7200 \n",
            "Epoch 00007 | Loss 0.9164 | Accuracy 0.7217 \n",
            "Epoch 00008 | Loss 0.8582 | Accuracy 0.7217 \n",
            "Epoch 00009 | Loss 0.7960 | Accuracy 0.7200 \n",
            "Epoch 00010 | Loss 0.7280 | Accuracy 0.7267 \n",
            "Epoch 00011 | Loss 0.6751 | Accuracy 0.7250 \n",
            "Epoch 00012 | Loss 0.5892 | Accuracy 0.7267 \n",
            "Epoch 00013 | Loss 0.5181 | Accuracy 0.7400 \n",
            "Epoch 00014 | Loss 0.4594 | Accuracy 0.7333 \n",
            "Epoch 00015 | Loss 0.4039 | Accuracy 0.7417 \n",
            "Epoch 00016 | Loss 0.3491 | Accuracy 0.7583 \n",
            "Epoch 00017 | Loss 0.3077 | Accuracy 0.7683 \n",
            "Epoch 00018 | Loss 0.2667 | Accuracy 0.7783 \n",
            "Epoch 00019 | Loss 0.2189 | Accuracy 0.7850 \n",
            "Epoch 00020 | Loss 0.1884 | Accuracy 0.7950 \n",
            "Epoch 00021 | Loss 0.1657 | Accuracy 0.7933 \n",
            "Epoch 00022 | Loss 0.1374 | Accuracy 0.7750 \n",
            "Epoch 00023 | Loss 0.1145 | Accuracy 0.7733 \n",
            "Epoch 00024 | Loss 0.0991 | Accuracy 0.7667 \n",
            "Epoch 00025 | Loss 0.0810 | Accuracy 0.7683 \n",
            "Epoch 00026 | Loss 0.0689 | Accuracy 0.7600 \n",
            "Epoch 00027 | Loss 0.0553 | Accuracy 0.7533 \n",
            "Epoch 00028 | Loss 0.0440 | Accuracy 0.7517 \n",
            "Epoch 00029 | Loss 0.0366 | Accuracy 0.7467 \n",
            "Epoch 00030 | Loss 0.0312 | Accuracy 0.7450 \n",
            "Early stopping at epoch=31\n",
            "Validation Accuracy: 0.745\n",
            "Trial 7/10: Hyperparameters - {'in_size': 478, 'hid_size': 8, 'out_size': 3, 'num_heads': 8, 'num_layers': 4, 'dropout': 0.1}\n",
            "Early stopping monitoring on\n",
            "Epoch 00000 | Loss 3.1842 | Accuracy 0.4617 \n",
            "Epoch 00001 | Loss 2.7207 | Accuracy 0.4883 \n",
            "Epoch 00002 | Loss 2.0203 | Accuracy 0.5550 \n",
            "Epoch 00003 | Loss 1.3537 | Accuracy 0.6017 \n",
            "Epoch 00004 | Loss 1.0222 | Accuracy 0.2550 \n",
            "Epoch 00005 | Loss 1.0068 | Accuracy 0.6967 \n",
            "Epoch 00006 | Loss 0.9923 | Accuracy 0.6950 \n",
            "Epoch 00007 | Loss 0.9334 | Accuracy 0.6733 \n",
            "Epoch 00008 | Loss 0.9666 | Accuracy 0.7133 \n",
            "Epoch 00009 | Loss 0.8496 | Accuracy 0.7167 \n",
            "Epoch 00010 | Loss 0.7304 | Accuracy 0.6233 \n",
            "Epoch 00011 | Loss 0.7378 | Accuracy 0.7167 \n",
            "Epoch 00012 | Loss 0.7035 | Accuracy 0.6900 \n",
            "Epoch 00013 | Loss 0.7163 | Accuracy 0.7167 \n",
            "Epoch 00014 | Loss 0.5661 | Accuracy 0.7217 \n",
            "Epoch 00015 | Loss 0.5147 | Accuracy 0.7183 \n",
            "Epoch 00016 | Loss 0.4024 | Accuracy 0.7267 \n",
            "Epoch 00017 | Loss 0.3667 | Accuracy 0.7267 \n",
            "Epoch 00018 | Loss 0.3174 | Accuracy 0.7383 \n",
            "Epoch 00019 | Loss 0.2986 | Accuracy 0.7467 \n",
            "Epoch 00020 | Loss 0.2522 | Accuracy 0.7483 \n",
            "Epoch 00021 | Loss 0.2797 | Accuracy 0.7400 \n",
            "Epoch 00022 | Loss 0.2127 | Accuracy 0.7683 \n",
            "Epoch 00023 | Loss 0.1724 | Accuracy 0.7650 \n",
            "Epoch 00024 | Loss 0.1622 | Accuracy 0.7550 \n",
            "Epoch 00025 | Loss 0.1580 | Accuracy 0.7433 \n",
            "Epoch 00026 | Loss 0.1198 | Accuracy 0.7500 \n",
            "Epoch 00027 | Loss 0.1018 | Accuracy 0.7567 \n",
            "Epoch 00028 | Loss 0.0842 | Accuracy 0.7733 \n",
            "Early stopping at epoch=29\n",
            "Validation Accuracy: 0.7733333333333333\n",
            "Trial 8/10: Hyperparameters - {'in_size': 478, 'hid_size': 32, 'out_size': 3, 'num_heads': 4, 'num_layers': 2, 'dropout': 0.01}\n",
            "Early stopping monitoring on\n",
            "Epoch 00000 | Loss 2.4767 | Accuracy 0.5217 \n",
            "Epoch 00001 | Loss 2.2616 | Accuracy 0.5217 \n",
            "Epoch 00002 | Loss 2.0370 | Accuracy 0.5183 \n",
            "Epoch 00003 | Loss 1.8093 | Accuracy 0.5183 \n",
            "Epoch 00004 | Loss 1.5885 | Accuracy 0.5167 \n",
            "Epoch 00005 | Loss 1.3973 | Accuracy 0.5167 \n",
            "Epoch 00006 | Loss 1.2387 | Accuracy 0.5183 \n",
            "Epoch 00007 | Loss 1.1234 | Accuracy 0.5267 \n",
            "Epoch 00008 | Loss 1.0253 | Accuracy 0.6033 \n",
            "Epoch 00009 | Loss 0.9505 | Accuracy 0.7150 \n",
            "Epoch 00010 | Loss 0.8871 | Accuracy 0.7300 \n",
            "Epoch 00011 | Loss 0.8352 | Accuracy 0.7200 \n",
            "Epoch 00012 | Loss 0.7903 | Accuracy 0.7250 \n",
            "Epoch 00013 | Loss 0.7556 | Accuracy 0.7233 \n",
            "Epoch 00014 | Loss 0.7188 | Accuracy 0.7250 \n",
            "Epoch 00015 | Loss 0.6797 | Accuracy 0.7250 \n",
            "Epoch 00016 | Loss 0.6453 | Accuracy 0.7400 \n",
            "Epoch 00017 | Loss 0.6116 | Accuracy 0.7383 \n",
            "Epoch 00018 | Loss 0.5851 | Accuracy 0.7433 \n",
            "Epoch 00019 | Loss 0.5566 | Accuracy 0.7417 \n",
            "Epoch 00020 | Loss 0.5265 | Accuracy 0.7417 \n",
            "Epoch 00021 | Loss 0.4978 | Accuracy 0.7517 \n",
            "Epoch 00022 | Loss 0.4791 | Accuracy 0.7483 \n",
            "Epoch 00023 | Loss 0.4485 | Accuracy 0.7500 \n",
            "Epoch 00024 | Loss 0.4315 | Accuracy 0.7533 \n",
            "Epoch 00025 | Loss 0.4029 | Accuracy 0.7533 \n",
            "Epoch 00026 | Loss 0.3924 | Accuracy 0.7633 \n",
            "Epoch 00027 | Loss 0.3698 | Accuracy 0.7600 \n",
            "Epoch 00028 | Loss 0.3546 | Accuracy 0.7750 \n",
            "Epoch 00029 | Loss 0.3331 | Accuracy 0.7800 \n",
            "Epoch 00030 | Loss 0.3202 | Accuracy 0.7833 \n",
            "Epoch 00031 | Loss 0.3072 | Accuracy 0.7717 \n",
            "Epoch 00032 | Loss 0.2961 | Accuracy 0.7683 \n",
            "Epoch 00033 | Loss 0.2829 | Accuracy 0.7717 \n",
            "Epoch 00034 | Loss 0.2676 | Accuracy 0.7750 \n",
            "Epoch 00035 | Loss 0.2586 | Accuracy 0.7750 \n",
            "Epoch 00036 | Loss 0.2506 | Accuracy 0.7817 \n",
            "Epoch 00037 | Loss 0.2376 | Accuracy 0.7817 \n",
            "Epoch 00038 | Loss 0.2349 | Accuracy 0.7850 \n",
            "Epoch 00039 | Loss 0.2261 | Accuracy 0.7850 \n",
            "Epoch 00040 | Loss 0.2187 | Accuracy 0.7833 \n",
            "Epoch 00041 | Loss 0.2090 | Accuracy 0.7800 \n",
            "Epoch 00042 | Loss 0.2037 | Accuracy 0.7783 \n",
            "Epoch 00043 | Loss 0.2013 | Accuracy 0.7817 \n",
            "Epoch 00044 | Loss 0.1941 | Accuracy 0.7800 \n",
            "Epoch 00045 | Loss 0.1880 | Accuracy 0.7767 \n",
            "Epoch 00046 | Loss 0.1853 | Accuracy 0.7833 \n",
            "Epoch 00047 | Loss 0.1797 | Accuracy 0.7833 \n",
            "Epoch 00048 | Loss 0.1735 | Accuracy 0.7833 \n",
            "Epoch 00049 | Loss 0.1737 | Accuracy 0.7817 \n",
            "Epoch 00050 | Loss 0.1682 | Accuracy 0.7800 \n",
            "Epoch 00051 | Loss 0.1611 | Accuracy 0.7867 \n",
            "Epoch 00052 | Loss 0.1593 | Accuracy 0.7867 \n",
            "Epoch 00053 | Loss 0.1576 | Accuracy 0.7867 \n",
            "Epoch 00054 | Loss 0.1511 | Accuracy 0.7883 \n",
            "Epoch 00055 | Loss 0.1475 | Accuracy 0.7883 \n",
            "Epoch 00056 | Loss 0.1450 | Accuracy 0.7917 \n",
            "Epoch 00057 | Loss 0.1425 | Accuracy 0.7900 \n",
            "Epoch 00058 | Loss 0.1370 | Accuracy 0.7917 \n",
            "Epoch 00059 | Loss 0.1353 | Accuracy 0.7883 \n",
            "Epoch 00060 | Loss 0.1312 | Accuracy 0.7900 \n",
            "Epoch 00061 | Loss 0.1252 | Accuracy 0.7900 \n",
            "Epoch 00062 | Loss 0.1247 | Accuracy 0.7900 \n",
            "Epoch 00063 | Loss 0.1210 | Accuracy 0.7917 \n",
            "Epoch 00064 | Loss 0.1202 | Accuracy 0.7933 \n",
            "Epoch 00065 | Loss 0.1151 | Accuracy 0.7933 \n",
            "Epoch 00066 | Loss 0.1153 | Accuracy 0.7933 \n",
            "Epoch 00067 | Loss 0.1131 | Accuracy 0.7933 \n",
            "Epoch 00068 | Loss 0.1090 | Accuracy 0.7917 \n",
            "Epoch 00069 | Loss 0.1090 | Accuracy 0.7900 \n",
            "Epoch 00070 | Loss 0.1056 | Accuracy 0.7883 \n",
            "Epoch 00071 | Loss 0.1025 | Accuracy 0.7883 \n",
            "Epoch 00072 | Loss 0.1019 | Accuracy 0.7917 \n",
            "Epoch 00073 | Loss 0.1029 | Accuracy 0.7917 \n",
            "Epoch 00074 | Loss 0.0986 | Accuracy 0.7900 \n",
            "Epoch 00075 | Loss 0.0983 | Accuracy 0.7867 \n",
            "Epoch 00076 | Loss 0.0980 | Accuracy 0.7883 \n",
            "Early stopping at epoch=77\n",
            "Validation Accuracy: 0.7883333333333333\n",
            "Trial 9/10: Hyperparameters - {'in_size': 478, 'hid_size': 8, 'out_size': 3, 'num_heads': 8, 'num_layers': 2, 'dropout': 0.001}\n",
            "Early stopping monitoring on\n",
            "Epoch 00000 | Loss 3.1733 | Accuracy 0.5917 \n",
            "Epoch 00001 | Loss 3.0362 | Accuracy 0.5983 \n",
            "Epoch 00002 | Loss 2.8838 | Accuracy 0.5717 \n",
            "Epoch 00003 | Loss 2.7095 | Accuracy 0.5633 \n",
            "Epoch 00004 | Loss 2.5155 | Accuracy 0.5483 \n",
            "Epoch 00005 | Loss 2.3053 | Accuracy 0.5400 \n",
            "Epoch 00006 | Loss 2.0892 | Accuracy 0.5300 \n",
            "Epoch 00007 | Loss 1.8761 | Accuracy 0.5217 \n",
            "Epoch 00008 | Loss 1.6771 | Accuracy 0.5167 \n",
            "Epoch 00009 | Loss 1.5011 | Accuracy 0.5183 \n",
            "Epoch 00010 | Loss 1.3538 | Accuracy 0.5300 \n",
            "Epoch 00011 | Loss 1.2332 | Accuracy 0.5583 \n",
            "Epoch 00012 | Loss 1.1408 | Accuracy 0.6083 \n",
            "Epoch 00013 | Loss 1.0687 | Accuracy 0.6833 \n",
            "Epoch 00014 | Loss 1.0093 | Accuracy 0.7333 \n",
            "Epoch 00015 | Loss 0.9646 | Accuracy 0.7433 \n",
            "Epoch 00016 | Loss 0.9239 | Accuracy 0.7300 \n",
            "Epoch 00017 | Loss 0.8931 | Accuracy 0.7250 \n",
            "Epoch 00018 | Loss 0.8670 | Accuracy 0.7200 \n",
            "Epoch 00019 | Loss 0.8412 | Accuracy 0.7267 \n",
            "Epoch 00020 | Loss 0.8158 | Accuracy 0.7217 \n",
            "Epoch 00021 | Loss 0.7930 | Accuracy 0.7217 \n",
            "Epoch 00022 | Loss 0.7705 | Accuracy 0.7283 \n",
            "Epoch 00023 | Loss 0.7484 | Accuracy 0.7350 \n",
            "Epoch 00024 | Loss 0.7264 | Accuracy 0.7433 \n",
            "Epoch 00025 | Loss 0.7071 | Accuracy 0.7400 \n",
            "Epoch 00026 | Loss 0.6857 | Accuracy 0.7383 \n",
            "Epoch 00027 | Loss 0.6652 | Accuracy 0.7400 \n",
            "Epoch 00028 | Loss 0.6464 | Accuracy 0.7433 \n",
            "Epoch 00029 | Loss 0.6249 | Accuracy 0.7433 \n",
            "Epoch 00030 | Loss 0.6064 | Accuracy 0.7450 \n",
            "Epoch 00031 | Loss 0.5874 | Accuracy 0.7467 \n",
            "Epoch 00032 | Loss 0.5718 | Accuracy 0.7533 \n",
            "Epoch 00033 | Loss 0.5537 | Accuracy 0.7550 \n",
            "Epoch 00034 | Loss 0.5331 | Accuracy 0.7583 \n",
            "Epoch 00035 | Loss 0.5180 | Accuracy 0.7600 \n",
            "Epoch 00036 | Loss 0.5003 | Accuracy 0.7617 \n",
            "Epoch 00037 | Loss 0.4855 | Accuracy 0.7583 \n",
            "Epoch 00038 | Loss 0.4703 | Accuracy 0.7600 \n",
            "Epoch 00039 | Loss 0.4548 | Accuracy 0.7633 \n",
            "Epoch 00040 | Loss 0.4426 | Accuracy 0.7683 \n",
            "Epoch 00041 | Loss 0.4295 | Accuracy 0.7667 \n",
            "Epoch 00042 | Loss 0.4164 | Accuracy 0.7733 \n",
            "Epoch 00043 | Loss 0.4067 | Accuracy 0.7683 \n",
            "Epoch 00044 | Loss 0.3944 | Accuracy 0.7667 \n",
            "Epoch 00045 | Loss 0.3806 | Accuracy 0.7650 \n",
            "Epoch 00046 | Loss 0.3699 | Accuracy 0.7717 \n",
            "Epoch 00047 | Loss 0.3607 | Accuracy 0.7717 \n",
            "Epoch 00048 | Loss 0.3516 | Accuracy 0.7750 \n",
            "Epoch 00049 | Loss 0.3412 | Accuracy 0.7767 \n",
            "Epoch 00050 | Loss 0.3326 | Accuracy 0.7833 \n",
            "Epoch 00051 | Loss 0.3255 | Accuracy 0.7883 \n",
            "Epoch 00052 | Loss 0.3187 | Accuracy 0.7867 \n",
            "Epoch 00053 | Loss 0.3097 | Accuracy 0.7883 \n",
            "Epoch 00054 | Loss 0.3024 | Accuracy 0.7883 \n",
            "Epoch 00055 | Loss 0.2965 | Accuracy 0.7883 \n",
            "Epoch 00056 | Loss 0.2896 | Accuracy 0.7900 \n",
            "Epoch 00057 | Loss 0.2846 | Accuracy 0.7900 \n",
            "Epoch 00058 | Loss 0.2786 | Accuracy 0.7900 \n",
            "Epoch 00059 | Loss 0.2725 | Accuracy 0.7867 \n",
            "Epoch 00060 | Loss 0.2685 | Accuracy 0.7850 \n",
            "Epoch 00061 | Loss 0.2632 | Accuracy 0.7850 \n",
            "Epoch 00062 | Loss 0.2582 | Accuracy 0.7850 \n",
            "Epoch 00063 | Loss 0.2541 | Accuracy 0.7817 \n",
            "Epoch 00064 | Loss 0.2490 | Accuracy 0.7800 \n",
            "Epoch 00065 | Loss 0.2453 | Accuracy 0.7800 \n",
            "Epoch 00066 | Loss 0.2404 | Accuracy 0.7783 \n",
            "Epoch 00067 | Loss 0.2366 | Accuracy 0.7767 \n",
            "Epoch 00068 | Loss 0.2332 | Accuracy 0.7767 \n",
            "Epoch 00069 | Loss 0.2294 | Accuracy 0.7767 \n",
            "Epoch 00070 | Loss 0.2262 | Accuracy 0.7767 \n",
            "Epoch 00071 | Loss 0.2228 | Accuracy 0.7767 \n",
            "Epoch 00072 | Loss 0.2188 | Accuracy 0.7767 \n",
            "Epoch 00073 | Loss 0.2171 | Accuracy 0.7767 \n",
            "Epoch 00074 | Loss 0.2132 | Accuracy 0.7783 \n",
            "Epoch 00075 | Loss 0.2103 | Accuracy 0.7783 \n",
            "Epoch 00076 | Loss 0.2065 | Accuracy 0.7767 \n",
            "Epoch 00077 | Loss 0.2046 | Accuracy 0.7750 \n",
            "Epoch 00078 | Loss 0.2019 | Accuracy 0.7750 \n",
            "Epoch 00079 | Loss 0.1991 | Accuracy 0.7750 \n",
            "Epoch 00080 | Loss 0.1961 | Accuracy 0.7750 \n",
            "Epoch 00081 | Loss 0.1936 | Accuracy 0.7750 \n",
            "Epoch 00082 | Loss 0.1914 | Accuracy 0.7750 \n",
            "Epoch 00083 | Loss 0.1885 | Accuracy 0.7767 \n",
            "Epoch 00084 | Loss 0.1889 | Accuracy 0.7767 \n",
            "Epoch 00085 | Loss 0.1829 | Accuracy 0.7767 \n",
            "Epoch 00086 | Loss 0.1815 | Accuracy 0.7767 \n",
            "Epoch 00087 | Loss 0.1802 | Accuracy 0.7767 \n",
            "Epoch 00088 | Loss 0.1779 | Accuracy 0.7767 \n",
            "Epoch 00089 | Loss 0.1743 | Accuracy 0.7750 \n",
            "Epoch 00090 | Loss 0.1719 | Accuracy 0.7767 \n",
            "Epoch 00091 | Loss 0.1704 | Accuracy 0.7767 \n",
            "Epoch 00092 | Loss 0.1676 | Accuracy 0.7767 \n",
            "Epoch 00093 | Loss 0.1659 | Accuracy 0.7800 \n",
            "Epoch 00094 | Loss 0.1645 | Accuracy 0.7800 \n",
            "Epoch 00095 | Loss 0.1620 | Accuracy 0.7800 \n",
            "Epoch 00096 | Loss 0.1615 | Accuracy 0.7800 \n",
            "Epoch 00097 | Loss 0.1583 | Accuracy 0.7817 \n",
            "Epoch 00098 | Loss 0.1573 | Accuracy 0.7817 \n",
            "Epoch 00099 | Loss 0.1553 | Accuracy 0.7817 \n",
            "Epoch 00100 | Loss 0.1529 | Accuracy 0.7833 \n",
            "Epoch 00101 | Loss 0.1519 | Accuracy 0.7833 \n",
            "Epoch 00102 | Loss 0.1498 | Accuracy 0.7833 \n",
            "Epoch 00103 | Loss 0.1481 | Accuracy 0.7833 \n",
            "Epoch 00104 | Loss 0.1458 | Accuracy 0.7833 \n",
            "Epoch 00105 | Loss 0.1465 | Accuracy 0.7833 \n",
            "Epoch 00106 | Loss 0.1431 | Accuracy 0.7833 \n",
            "Epoch 00107 | Loss 0.1418 | Accuracy 0.7833 \n",
            "Epoch 00108 | Loss 0.1410 | Accuracy 0.7833 \n",
            "Epoch 00109 | Loss 0.1391 | Accuracy 0.7833 \n",
            "Epoch 00110 | Loss 0.1365 | Accuracy 0.7833 \n",
            "Epoch 00111 | Loss 0.1332 | Accuracy 0.7833 \n",
            "Epoch 00112 | Loss 0.1332 | Accuracy 0.7833 \n",
            "Epoch 00113 | Loss 0.1318 | Accuracy 0.7817 \n",
            "Epoch 00114 | Loss 0.1288 | Accuracy 0.7817 \n",
            "Epoch 00115 | Loss 0.1268 | Accuracy 0.7817 \n",
            "Epoch 00116 | Loss 0.1265 | Accuracy 0.7833 \n",
            "Epoch 00117 | Loss 0.1234 | Accuracy 0.7817 \n",
            "Epoch 00118 | Loss 0.1229 | Accuracy 0.7817 \n",
            "Epoch 00119 | Loss 0.1199 | Accuracy 0.7867 \n",
            "Epoch 00120 | Loss 0.1192 | Accuracy 0.7850 \n",
            "Epoch 00121 | Loss 0.1160 | Accuracy 0.7833 \n",
            "Epoch 00122 | Loss 0.1153 | Accuracy 0.7833 \n",
            "Epoch 00123 | Loss 0.1141 | Accuracy 0.7833 \n",
            "Epoch 00124 | Loss 0.1115 | Accuracy 0.7850 \n",
            "Epoch 00125 | Loss 0.1102 | Accuracy 0.7850 \n",
            "Epoch 00126 | Loss 0.1091 | Accuracy 0.7850 \n",
            "Epoch 00127 | Loss 0.1095 | Accuracy 0.7883 \n",
            "Epoch 00128 | Loss 0.1065 | Accuracy 0.7867 \n",
            "Epoch 00129 | Loss 0.1049 | Accuracy 0.7867 \n",
            "Epoch 00130 | Loss 0.1034 | Accuracy 0.7867 \n",
            "Epoch 00131 | Loss 0.1025 | Accuracy 0.7883 \n",
            "Epoch 00132 | Loss 0.1012 | Accuracy 0.7883 \n",
            "Epoch 00133 | Loss 0.0994 | Accuracy 0.7883 \n",
            "Epoch 00134 | Loss 0.0997 | Accuracy 0.7867 \n",
            "Epoch 00135 | Loss 0.0959 | Accuracy 0.7850 \n",
            "Epoch 00136 | Loss 0.0974 | Accuracy 0.7883 \n",
            "Epoch 00137 | Loss 0.0961 | Accuracy 0.7867 \n",
            "Epoch 00138 | Loss 0.0948 | Accuracy 0.7867 \n",
            "Epoch 00139 | Loss 0.0966 | Accuracy 0.7883 \n",
            "Epoch 00140 | Loss 0.0935 | Accuracy 0.7883 \n",
            "Epoch 00141 | Loss 0.0917 | Accuracy 0.7900 \n",
            "Epoch 00142 | Loss 0.0914 | Accuracy 0.7900 \n",
            "Epoch 00143 | Loss 0.0922 | Accuracy 0.7883 \n",
            "Epoch 00144 | Loss 0.0899 | Accuracy 0.7883 \n",
            "Epoch 00145 | Loss 0.0884 | Accuracy 0.7883 \n",
            "Epoch 00146 | Loss 0.0872 | Accuracy 0.7883 \n",
            "Epoch 00147 | Loss 0.0874 | Accuracy 0.7917 \n",
            "Epoch 00148 | Loss 0.0873 | Accuracy 0.7900 \n",
            "Epoch 00149 | Loss 0.0863 | Accuracy 0.7900 \n",
            "Epoch 00150 | Loss 0.0846 | Accuracy 0.7883 \n",
            "Epoch 00151 | Loss 0.0828 | Accuracy 0.7867 \n",
            "Epoch 00152 | Loss 0.0844 | Accuracy 0.7850 \n",
            "Epoch 00153 | Loss 0.0819 | Accuracy 0.7850 \n",
            "Early stopping at epoch=154\n",
            "Validation Accuracy: 0.785\n",
            "Trial 10/10: Hyperparameters - {'in_size': 478, 'hid_size': 8, 'out_size': 3, 'num_heads': 8, 'num_layers': 2, 'dropout': 0.1}\n",
            "Early stopping monitoring on\n",
            "Epoch 00000 | Loss 3.1835 | Accuracy 0.4467 \n",
            "Epoch 00001 | Loss 3.0461 | Accuracy 0.4150 \n",
            "Epoch 00002 | Loss 2.8941 | Accuracy 0.3933 \n",
            "Epoch 00003 | Loss 2.7190 | Accuracy 0.3917 \n",
            "Epoch 00004 | Loss 2.5368 | Accuracy 0.3917 \n",
            "Epoch 00005 | Loss 2.3234 | Accuracy 0.3917 \n",
            "Epoch 00006 | Loss 2.1286 | Accuracy 0.3917 \n",
            "Epoch 00007 | Loss 1.9254 | Accuracy 0.3917 \n",
            "Epoch 00008 | Loss 1.7438 | Accuracy 0.3917 \n",
            "Epoch 00009 | Loss 1.5894 | Accuracy 0.3917 \n",
            "Epoch 00010 | Loss 1.4374 | Accuracy 0.3917 \n",
            "Epoch 00011 | Loss 1.3391 | Accuracy 0.3917 \n",
            "Epoch 00012 | Loss 1.2246 | Accuracy 0.4350 \n",
            "Epoch 00013 | Loss 1.1420 | Accuracy 0.4950 \n",
            "Epoch 00014 | Loss 1.1006 | Accuracy 0.6000 \n",
            "Epoch 00015 | Loss 1.0217 | Accuracy 0.7050 \n",
            "Epoch 00016 | Loss 1.0107 | Accuracy 0.7150 \n",
            "Epoch 00017 | Loss 0.9611 | Accuracy 0.7267 \n",
            "Epoch 00018 | Loss 0.9382 | Accuracy 0.6833 \n",
            "Epoch 00019 | Loss 0.9192 | Accuracy 0.6317 \n",
            "Epoch 00020 | Loss 0.8998 | Accuracy 0.6133 \n",
            "Epoch 00021 | Loss 0.8958 | Accuracy 0.6517 \n",
            "Epoch 00022 | Loss 0.8590 | Accuracy 0.6983 \n",
            "Epoch 00023 | Loss 0.8262 | Accuracy 0.7300 \n",
            "Epoch 00024 | Loss 0.8308 | Accuracy 0.7367 \n",
            "Epoch 00025 | Loss 0.8160 | Accuracy 0.7267 \n",
            "Epoch 00026 | Loss 0.7770 | Accuracy 0.7183 \n",
            "Epoch 00027 | Loss 0.7824 | Accuracy 0.7250 \n",
            "Epoch 00028 | Loss 0.7322 | Accuracy 0.7233 \n",
            "Epoch 00029 | Loss 0.7170 | Accuracy 0.7267 \n",
            "Epoch 00030 | Loss 0.7058 | Accuracy 0.7300 \n",
            "Epoch 00031 | Loss 0.6849 | Accuracy 0.7333 \n",
            "Epoch 00032 | Loss 0.6725 | Accuracy 0.7400 \n",
            "Epoch 00033 | Loss 0.6589 | Accuracy 0.7417 \n",
            "Epoch 00034 | Loss 0.6496 | Accuracy 0.7383 \n",
            "Epoch 00035 | Loss 0.6430 | Accuracy 0.7383 \n",
            "Epoch 00036 | Loss 0.6186 | Accuracy 0.7400 \n",
            "Epoch 00037 | Loss 0.5914 | Accuracy 0.7450 \n",
            "Epoch 00038 | Loss 0.5720 | Accuracy 0.7533 \n",
            "Epoch 00039 | Loss 0.5438 | Accuracy 0.7467 \n",
            "Epoch 00040 | Loss 0.5540 | Accuracy 0.7500 \n",
            "Epoch 00041 | Loss 0.5370 | Accuracy 0.7550 \n",
            "Epoch 00042 | Loss 0.5231 | Accuracy 0.7550 \n",
            "Epoch 00043 | Loss 0.5124 | Accuracy 0.7550 \n",
            "Epoch 00044 | Loss 0.4944 | Accuracy 0.7550 \n",
            "Epoch 00045 | Loss 0.4914 | Accuracy 0.7583 \n",
            "Epoch 00046 | Loss 0.4816 | Accuracy 0.7583 \n",
            "Epoch 00047 | Loss 0.4634 | Accuracy 0.7600 \n",
            "Epoch 00048 | Loss 0.4221 | Accuracy 0.7667 \n",
            "Epoch 00049 | Loss 0.4079 | Accuracy 0.7700 \n",
            "Epoch 00050 | Loss 0.4339 | Accuracy 0.7683 \n",
            "Epoch 00051 | Loss 0.4011 | Accuracy 0.7717 \n",
            "Epoch 00052 | Loss 0.4040 | Accuracy 0.7750 \n",
            "Epoch 00053 | Loss 0.3867 | Accuracy 0.7750 \n",
            "Epoch 00054 | Loss 0.3641 | Accuracy 0.7783 \n",
            "Epoch 00055 | Loss 0.3680 | Accuracy 0.7783 \n",
            "Epoch 00056 | Loss 0.3592 | Accuracy 0.7800 \n",
            "Epoch 00057 | Loss 0.3367 | Accuracy 0.7783 \n",
            "Epoch 00058 | Loss 0.3335 | Accuracy 0.7800 \n",
            "Epoch 00059 | Loss 0.3347 | Accuracy 0.7783 \n",
            "Epoch 00060 | Loss 0.3243 | Accuracy 0.7783 \n",
            "Epoch 00061 | Loss 0.3153 | Accuracy 0.7783 \n",
            "Epoch 00062 | Loss 0.3076 | Accuracy 0.7783 \n",
            "Epoch 00063 | Loss 0.3082 | Accuracy 0.7817 \n",
            "Epoch 00064 | Loss 0.3006 | Accuracy 0.7833 \n",
            "Epoch 00065 | Loss 0.2911 | Accuracy 0.7850 \n",
            "Epoch 00066 | Loss 0.2899 | Accuracy 0.7817 \n",
            "Epoch 00067 | Loss 0.2815 | Accuracy 0.7817 \n",
            "Epoch 00068 | Loss 0.2921 | Accuracy 0.7800 \n",
            "Epoch 00069 | Loss 0.2788 | Accuracy 0.7800 \n",
            "Epoch 00070 | Loss 0.2681 | Accuracy 0.7800 \n",
            "Epoch 00071 | Loss 0.2640 | Accuracy 0.7800 \n",
            "Epoch 00072 | Loss 0.2529 | Accuracy 0.7833 \n",
            "Epoch 00073 | Loss 0.2597 | Accuracy 0.7867 \n",
            "Epoch 00074 | Loss 0.2716 | Accuracy 0.7883 \n",
            "Epoch 00075 | Loss 0.2565 | Accuracy 0.7950 \n",
            "Epoch 00076 | Loss 0.2301 | Accuracy 0.7917 \n",
            "Epoch 00077 | Loss 0.2412 | Accuracy 0.7917 \n",
            "Epoch 00078 | Loss 0.2459 | Accuracy 0.7900 \n",
            "Epoch 00079 | Loss 0.2420 | Accuracy 0.7883 \n",
            "Epoch 00080 | Loss 0.2362 | Accuracy 0.7833 \n",
            "Epoch 00081 | Loss 0.2292 | Accuracy 0.7833 \n",
            "Epoch 00082 | Loss 0.2243 | Accuracy 0.7850 \n",
            "Epoch 00083 | Loss 0.2066 | Accuracy 0.7833 \n",
            "Epoch 00084 | Loss 0.2195 | Accuracy 0.7833 \n",
            "Epoch 00085 | Loss 0.2223 | Accuracy 0.7900 \n",
            "Epoch 00086 | Loss 0.2132 | Accuracy 0.7917 \n",
            "Epoch 00087 | Loss 0.2046 | Accuracy 0.7850 \n",
            "Epoch 00088 | Loss 0.2189 | Accuracy 0.7850 \n",
            "Epoch 00089 | Loss 0.2251 | Accuracy 0.7867 \n",
            "Epoch 00090 | Loss 0.2113 | Accuracy 0.7783 \n",
            "Epoch 00091 | Loss 0.2027 | Accuracy 0.7783 \n",
            "Epoch 00092 | Loss 0.1924 | Accuracy 0.7800 \n",
            "Epoch 00093 | Loss 0.2014 | Accuracy 0.7800 \n",
            "Epoch 00094 | Loss 0.2060 | Accuracy 0.7850 \n",
            "Epoch 00095 | Loss 0.1887 | Accuracy 0.7833 \n",
            "Epoch 00096 | Loss 0.1836 | Accuracy 0.7900 \n",
            "Epoch 00097 | Loss 0.1865 | Accuracy 0.7900 \n",
            "Epoch 00098 | Loss 0.1806 | Accuracy 0.7800 \n",
            "Epoch 00099 | Loss 0.2016 | Accuracy 0.7800 \n",
            "Epoch 00100 | Loss 0.1785 | Accuracy 0.7833 \n",
            "Epoch 00101 | Loss 0.1765 | Accuracy 0.7800 \n",
            "Epoch 00102 | Loss 0.1740 | Accuracy 0.7817 \n",
            "Epoch 00103 | Loss 0.1779 | Accuracy 0.7767 \n",
            "Epoch 00104 | Loss 0.1804 | Accuracy 0.7733 \n",
            "Epoch 00105 | Loss 0.1728 | Accuracy 0.7733 \n",
            "Epoch 00106 | Loss 0.1575 | Accuracy 0.7767 \n",
            "Epoch 00107 | Loss 0.1760 | Accuracy 0.7817 \n",
            "Epoch 00108 | Loss 0.1673 | Accuracy 0.7900 \n",
            "Epoch 00109 | Loss 0.1581 | Accuracy 0.7900 \n",
            "Epoch 00110 | Loss 0.1675 | Accuracy 0.7900 \n",
            "Epoch 00111 | Loss 0.1604 | Accuracy 0.7800 \n",
            "Epoch 00112 | Loss 0.1502 | Accuracy 0.7650 \n",
            "Epoch 00113 | Loss 0.1566 | Accuracy 0.7667 \n",
            "Epoch 00114 | Loss 0.1599 | Accuracy 0.7683 \n",
            "Epoch 00115 | Loss 0.1536 | Accuracy 0.7733 \n",
            "Epoch 00116 | Loss 0.1573 | Accuracy 0.7800 \n",
            "Epoch 00117 | Loss 0.1455 | Accuracy 0.7867 \n",
            "Epoch 00118 | Loss 0.1490 | Accuracy 0.7933 \n",
            "Epoch 00119 | Loss 0.1461 | Accuracy 0.7950 \n",
            "Epoch 00120 | Loss 0.1365 | Accuracy 0.7900 \n",
            "Epoch 00121 | Loss 0.1481 | Accuracy 0.7750 \n",
            "Epoch 00122 | Loss 0.1270 | Accuracy 0.7700 \n",
            "Early stopping at epoch=123\n",
            "Validation Accuracy: 0.77\n",
            "Best Hyperparameters: {'in_size': 478, 'hid_size': 32, 'out_size': 3, 'num_heads': 4, 'num_layers': 2, 'dropout': 0.01}\n",
            "Best Accuracy: 0.7883333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5uqeDqJkKRqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aRMIlLhXKRfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "23X-XFFOKRcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "enNiVzJyKRWe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}